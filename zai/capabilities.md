# Thinking Mode

> GLM-4.7 offers multiple thinking modes for different scenarios. The sections below explain how to enable each mode, key considerations, and example usage.

## **Default Thinking Behaviour**

Thinking is activated by default in GLM-4.7, different from the default hybrid thinking in GLM-4.6.

> If you want to disable thinking, use:

```bash  theme={null}
"thinking": {
    "type": "disabled"
}
```

## **Interleaved thinking**

We support **interleaved thinking** by default (supported since GLM-4.5), allowing GLM to think between tool calls and after receiving tool results. This enables more complex, step-by-step reasoning: interpreting each tool output before deciding what to do next, chaining multiple tool calls with reasoning steps, and making finer-grained decisions based on intermediate results.

<Tip>
  When using interleaved thinking with tools, **thinking blocks should be explicitly preserved and returned together with the tool results.**
</Tip>

The detailed interleaved thinking process is as follows.

![Description](https://cdn.bigmodel.cn/markdown/1766025484368img_v3_02t3_4677ac48-b748-44d8-a56f-8cbd599b51ag.jpg?attname=img_v3_02t3_4677ac48-b748-44d8-a56f-8cbd599b51ag.jpg)

## **Preserved thinking**

**GLM-4.7 introduces a new capability**Â in coding scenarios: the model can retainÂ **reasoning content from previous assistant turns**Â in the context. This helps preserve reasoning continuity and conversation integrity, improves model performance, and increases cache hit ratesâ€”saving tokens in real tasks.

<Check>
  This capability is **enabled by default** on the **Coding Plan endpoint** and **disabled by default** on the **standard API endpoint**. If you want to enable **Preserved Thinking** in your product (primarily recommended for coding/agent scenarios), you can turn it on for the API endpoint by setting **"clear\_thinking": false**, and **you must return the complete**, unmodified reasoning\_content back to the API.

  All consecutive reasoning\_content blocks must **exactly match the original sequence** generated by the model during the initial request. Do not reorder or edit these blocks; otherwise, performance may degrade and cache hit rates may be affected.
</Check>

The detailed Preserved thinking process is as follows.

![Description](https://cdn.bigmodel.cn/markdown/176641919972020251222-235942.jpeg?attname=20251222-235942.jpeg)

## Turn-level Thinking

â€œTurn-level Thinkingâ€ is a capability that **lets you control reasoning computation on a per-turn basis**: within the same session, each request can independently choose to enable or disable thinking. This is a new capability introduced in GLM-4.7, with the following advantages:

* **More flexible cost/latency control:** For lightweight turns like â€œasking a factâ€ or â€œtweaking wording,â€ you can disable thinking to get faster responses; for heavier tasks like â€œcomplex planning,â€ â€œmulti-constraint reasoning,â€ or â€œcode debugging,â€ you can enable thinking to improve accuracy and stability.
* **Smoother multi-turn experience:** The thinking switch can be toggled at any point within a session. The model stays coherent across turns and keeps a consistent output style, making it feel â€œsmarter when things are hard, faster when things are simple.â€
* **Better for agent/tool-use scenarios:** On turns that require quick tool execution, you can reduce reasoning overhead; on turns that require making decisions based on tool results, you can turn on deeper thinkingâ€”dynamically balancing efficiency and quality.

## Example Usage

This applies to both **Interleaved Thinking** and **Preserved Thinking**â€”no manual differentiation is required. **Remember to return the historical** `reasoning_content`**to keep the reasoning coherent.**

```python  theme={null}
""""Interleaved Thinking + Tool Calling Example"""

import json
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.z.ai/api/paas/v4/",
)

tools = [{"type": "function", "function": {
    "name": "get_weather",
    "description": "Get weather information",
    "parameters": {"type": "object", "properties": {"city": {"type": "string"}}, "required": ["city"]},
}}]

messages = [
    {"role": "system", "content": "You are an assistant"},
    {"role": "user", "content": "What's the weather like in Beijing?"},
]

# Round 1: the model reasons and then calls a tool
response = client.chat.completions.create(model="glm-4.7", messages=messages, tools=tools, stream=True, extra_body={
        "thinking":{
        "type":"enabled",
        "clear_thinking": False  # False for Preserved Thinking
    }})
reasoning, content, tool_calls = "", "", []
for chunk in response:
    delta = chunk.choices[0].delta
    if hasattr(delta, "reasoning_content") and delta.reasoning_content:
        reasoning += delta.reasoning_content
    if hasattr(delta, "content") and delta.content:
        content += delta.content
    if hasattr(delta, "tool_calls") and delta.tool_calls:
        for tc in delta.tool_calls:
            if tc.index >= len(tool_calls):
                tool_calls.append({"id": tc.id, "function": {"name": "", "arguments": ""}})
            if tc.function.name:
                tool_calls[tc.index]["function"]["name"] = tc.function.name
            if tc.function.arguments:
                tool_calls[tc.index]["function"]["arguments"] += tc.function.arguments

print(f"Reasoning: {reasoning}\nTool calls: {tool_calls}")

# Key: return reasoning_content to keep the reasoning coherent
messages.append({"role": "assistant", "content": content, "reasoning_content": reasoning,
                 "tool_calls": [{"id": tc["id"], "type": "function", "function": tc["function"]} for tc in tool_calls]})
messages.append({"role": "tool", "tool_call_id": tool_calls[0]["id"],
                 "content": json.dumps({"weather": "Sunny", "temp": "25Â°C"})})

# Round 2: the model continues reasoning based on the tool result and responds
response = client.chat.completions.create(model="glm-4.7", messages=messages, tools=tools, stream=True, extra_body={
        "thinking":{
        "type":"enabled",
        "clear_thinking": False # False for Preserved Thinking
    }})
reasoning, content = "", ""
for chunk in response:
    delta = chunk.choices[0].delta
    if hasattr(delta, "reasoning_content") and delta.reasoning_content:
        reasoning += delta.reasoning_content
    if hasattr(delta, "content") and delta.content:
        content += delta.content

print(f"Reasoning: {reasoning}\nReply: {content}")
```


---

# Deep Thinking

<Tip>
  Deep Thinking is an advanced reasoning feature that enables Chain of Thought mechanisms, allowing the model to perform deep analysis and reasoning before answering questions. This approach significantly improves the model's accuracy and interpretability in complex tasks, particularly suitable for scenarios requiring multi-step reasoning, logical analysis, and problem-solving.
</Tip>

## Features

The Deep Thinking feature currently supports the latest models in the GLM-4.5 and GLM-4.6 series. By enabling deep thinking, the model can:

* **Multi-step Reasoning**: Break down complex problems into multiple steps for gradual analysis and resolution
* **Logical Analysis**: Provide clear reasoning processes and logical chains
* **Improved Accuracy**: Reduce errors and improve answer quality through deep thinking
* **Enhanced Interpretability**: Display the thinking process to help users understand the model's reasoning logic
* **Intelligent Judgment**: The model automatically determines whether deep thinking is needed to optimize response efficiency

### Core Parameters

* **`thinking.type`**: Controls the deep thinking mode
  * `enabled` (default): Enable dynamic thinking, model automatically determines if deep thinking is needed
  * `disabled`: Disable deep thinking, provide direct answers
* **`model`**: Models that support deep thinking, such as `glm-4.6`, `glm-4.5`, `glm-4.5v`, etc.

## Code Examples

<Tabs>
  <Tab title="cURL">
    **Basic Call (Enable Deep Thinking)**

    ```bash  theme={null}
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "user",
                "content": "Explain in detail the basic principles of quantum computing and analyze its potential impact in the field of cryptography"
            }
        ],
        "thinking": {
            "type": "enabled"
        },
        "max_tokens": 4096,
        "temperature": 1.0
    }'
    ```

    **Streaming Call (Deep Thinking + Streaming Output)**

    ```bash  theme={null}
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "user",
                "content": "Design a recommendation system architecture for an e-commerce website, considering user behavior, product features, and real-time requirements"
            }
        ],
        "thinking": {
            "type": "enabled"
        },
        "stream": true,
        "max_tokens": 4096,
        "temperature": 1.0
    }'
    ```

    **Disable Deep Thinking**

    ```bash  theme={null}
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "user",
                "content": "How is the weather today?"
            }
        ],
        "thinking": {
            "type": "disabled"
        }
    }'
    ```
  </Tab>

  <Tab title="Python SDK">
    **Install SDK**

    ```bash  theme={null}
    # Install latest version
    pip install zai-sdk

    # Or specify version
    pip install zai-sdk==0.1.0
    ```

    **Verify Installation**

    ```python  theme={null}
    import zai
    print(zai.__version__)
    ```

    **Basic Call (Enable Deep Thinking)**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='your_api_key')

    # Create deep thinking request
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {"role": "user", "content": "Explain in detail the basic principles of quantum computing and analyze its potential impact in the field of cryptography"}
        ],
        thinking={
            "type": "enabled"  # Enable deep thinking mode
        },
        max_tokens=4096,
        temperature=1.0
    )

    print("Model response:")
    print(response.choices[0].message.content)
    print("\n---")
    print(response.choices[0].message.reasoning_content)
    ```

    **Streaming Call (Deep Thinking + Streaming Output)**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='your_api_key')

    # Create streaming deep thinking request
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {"role": "user", "content": "Design a recommendation system architecture for an e-commerce website, considering user behavior, product features, and real-time requirements"}
        ],
        thinking={
            "type": "enabled"  # Enable deep thinking mode
        },
        stream=True,  # Enable streaming output
        max_tokens=4096,
        temperature=1.0
    )

    # Process streaming response
    reasoning_content = ""
    thinking_phase = True

    for chunk in response:
        if not chunk.choices:
            continue
        
        delta = chunk.choices[0].delta
        
        # Process thinking process (if any)
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            reasoning_content += delta.reasoning_content
            if thinking_phase:
                print("ðŸ§  Thinking...", end="", flush=True)
                thinking_phase = False
            print(delta.reasoning_content, end="", flush=True)
        
        # Process answer content
        if hasattr(delta, 'content') and delta.content:
            if thinking_phase:
                print("\n\nðŸ’¡ Answer:")
                thinking_phase = False
            print(delta.content, end="", flush=True)

    ```

    **Disable Deep Thinking**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='your_api_key')

    # Disable deep thinking for quick response
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {"role": "user", "content": "How is the weather today?"}
        ],
        thinking={
            "type": "disabled"  # Disable deep thinking mode
        }
    )

    print(response.choices[0].message.content)
    ```
  </Tab>
</Tabs>

### Response Example

Response format with deep thinking enabled:

```json  theme={null}
{
  "created": 1677652288,
  "model": "glm-4.7",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Artificial intelligence has tremendous application prospects in medical diagnosis...",
        "reasoning_content": "Let me analyze this question from multiple angles. First, I need to consider the technical advantages of AI in medical diagnosis..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "completion_tokens": 239,
    "prompt_tokens": 8,
    "prompt_tokens_details": {
      "cached_tokens": 0
    },
    "total_tokens": 247
  }
}
```

## Best Practices

**Recommended scenarios to enable:**

* Complex problem analysis and solving
* Multi-step reasoning tasks
* Technical solution design
* Strategy planning and decision
* Academic research and analysis
* Creative writing and content creation

**Can be disabled scenarios:**

* Simple fact query
* Basic translation tasks
* Simple classification judgment
* Quick question and answer requirements

## Application scenarios

<CardGroup cols={2}>
  <Card title="Academic Research" icon="book">
    * Research method design
    * Data analysis and explanation
    * Theory deduction and proof
  </Card>

  <Card title="Technology Consulting" icon="code">
    * System architecture design
    * Technological scheme evaluation
    * Problem diagnosis and solution
  </Card>

  <Card title="Business Analysis" icon="chart-line">
    * Market trends analysis
    * Business model design
    * Investment decision support
  </Card>

  <Card title="Education Training" icon="users">
    * Complex concept explanation
    * Learning path planning
    * Knowledge system building
  </Card>
</CardGroup>

## Notes

1. **Response time**ï¼šEnable deep thinking will increase response time, particularly for complex tasks
2. **Token consumption**ï¼šThinking process will consume extra tokens, please manage your tokens
3. **Model support**ï¼šEnsure you're using models that support deep thinking
4. **Task matching**ï¼šChoose whether to enable deep thinking according to the task complexity
5. **Streaming output**ï¼šCombine streaming output to see the thinking process, improving user experience


---

# Streaming Messages

<Tip>
  Streaming Messages allow real-time content retrieval while the model generates responses, without waiting for the complete response to be generated. This approach can significantly improve user experience, especially when generating long text content, as users can immediately see output beginning to appear.
</Tip>

## Features

Streaming messages use an incremental generation mechanism, transmitting content in chunks in real-time during the generation process, rather than waiting for the complete response to be generated before returning it all at once. This mechanism allows developers to:

* **Real-time Response**: No need to wait for complete response, content displays progressively
* **Improved Experience**: Reduce user waiting time, provide instant feedback
* **Reduced Latency**: Content is transmitted as it's generated, reducing perceived latency
* **Flexible Processing**: Real-time processing and display during reception

### Core Parameter Description

* **`stream=True`**: Enable streaming output, must be set to `True`
* **`model`**: Models that support streaming output, such as `glm-4.7`, `glm-4.6`, `glm-4.5`, etc.

### Response Format Description

Streaming responses use Server-Sent Events (SSE) format, with each event containing:

* `choices[0].delta.content`: Incremental text content
* `choices[0].delta.reasoning_content`: Incremental reasoning content
* `choices[0].finish_reason`: Completion reason (only appears in the last chunk)
* `usage`: Token usage statistics (only appears in the last chunk)

## Code Examples

<Tabs>
  <Tab title="cURL">
    ```bash  theme={null}
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "user",
                "content": "Write a poem about spring"
            }
        ],
        "stream": true
    }'
    ```
  </Tab>

  <Tab title="Python">
    **Install SDK**

    ```bash  theme={null}
    # Install latest version
    pip install zai-sdk

    # Or specify version
    pip install zai-sdk==0.1.0
    ```

    **Verify Installation**

    ```python  theme={null}
    import zai
    print(zai.__version__)
    ```

    **Complete Example**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='Your API Key')

    # Create streaming message request
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {"role": "user", "content": "Write a poem about spring"}
        ],
        stream=True  # Enable streaming output
    )

    # Process streaming response
    full_content = ""
    for chunk in response:
        if not chunk.choices:
            continue
        
        delta = chunk.choices[0].delta
        
        # Handle incremental content
        if hasattr(delta, 'content') and delta.content:
            full_content += delta.content
            print(delta.content, end="", flush=True)
        
        # Check if completed
        if chunk.choices[0].finish_reason:
            print(f"\n\nCompletion reason: {chunk.choices[0].finish_reason}")
            if hasattr(chunk, 'usage') and chunk.usage:
                print(f"Token usage: Input {chunk.usage.prompt_tokens}, Output {chunk.usage.completion_tokens}")

    print(f"\n\nComplete content:\n{full_content}")
    ```
  </Tab>
</Tabs>

### Response Example

The streaming response format is as follows:

```
data: {"id":"1","created":1677652288,"model":"glm-4.7","choices":[{"index":0,"delta":{"content":"Spring"},"finish_reason":null}]}

data: {"id":"1","created":1677652288,"model":"glm-4.7","choices":[{"index":0,"delta":{"content":" comes"},"finish_reason":null}]}

data: {"id":"1","created":1677652288,"model":"glm-4.7","choices":[{"index":0,"delta":{"content":" with"},"finish_reason":null}]}

...

data: {"id":"1","created":1677652288,"model":"glm-4.7","choices":[{"index":0,"finish_reason":"stop","delta":{"role":"assistant","content":""}}],"usage":{"prompt_tokens":8,"completion_tokens":262,"total_tokens":270,"prompt_tokens_details":{"cached_tokens":0}}}

data: [DONE]
```

## Application Scenarios

<CardGroup cols={2}>
  <Card title="Chat Applications" icon="headset">
    * Real-time conversation experience
    * Character-by-character reply display
    * Reduced waiting time
  </Card>

  <Card title="Content Generation" icon="feather">
    * Article writing assistant
    * Code generation tools
    * Creative content creation
  </Card>

  <Card title="Educational Applications" icon="book">
    * Online Q\&A systems
    * Learning assistance tools
    * Knowledge Q\&A platforms
  </Card>

  <Card title="Customer Service Systems" icon="users">
    * Intelligent customer service bots
    * Real-time problem solving
    * User support systems
  </Card>
</CardGroup>


---

# Tool Streaming Output

<Tip>
  Stream Tool Call is a unique feature of Z.ai's latest model GLM-4.6, allowing real-time access to reasoning processes, response content, and tool call information during tool invocation, providing better user experience and real-time feedback.
</Tip>

## Features

Tool calling in the latest GLM-4.6 model now supports streaming output for responses. This allows developers to stream tool usage parameters without buffering or JSON validation when calling `chat.completions`, reducing call latency and providing better user experience.

### Core Parameter Description

* **`stream=True`**: Enable streaming output, must be set to `True`
* **`tool_stream=True`**: Enable tool call streaming output
* **`model`**: Use a model that supports tool calling, limited to `glm-4.6`

### Response Parameter Description

The `delta` object in streaming responses contains the following fields:

* **`reasoning_content`**: Text content of the model's reasoning process
* **`content`**: Text content of the model's response
* **`tool_calls`**: Tool call information, including function names and parameters

## Code Examples

By setting the `tool_stream=True` parameter, you can enable streaming tool call functionality:

<Tabs>
  <Tab title="Python SDK">
    **Install SDK**

    ```bash  theme={null}
    # Install latest version
    pip install zai-sdk

    # Or specify version
    pip install zai-sdk==0.1.0
    ```

    **Verify Installation**

    ```python  theme={null}
    import zai
    print(zai.__version__)
    ```

    **Complete Example**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='Your API Key')

    # Create streaming tool call request
    response = client.chat.completions.create(
        model="glm-4.7",  # Use model that supports tool calling
        messages=[
            {"role": "user", "content": "How's the weather in Beijing?"},
        ],
        tools=[
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get current weather conditions for a specified location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string", "description": "City, e.g.: Beijing, Shanghai"},
                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
                        },
                        "required": ["location"]
                    }
                }
            }
        ],
        stream=True,        # Enable streaming output
        tool_stream=True    # Enable tool call streaming output
    )

    # Initialize variables to collect streaming data
    reasoning_content = ""      # Reasoning process content
    content = ""               # Response content
    final_tool_calls = {}      # Tool call information
    reasoning_started = False  # Reasoning process start flag
    content_started = False    # Content output start flag

    # Process streaming response
    for chunk in response:
        if not chunk.choices:
            continue

        delta = chunk.choices[0].delta

        # Handle streaming reasoning process output
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            if not reasoning_started and delta.reasoning_content.strip():
                print("\nðŸ§  Thinking Process:")
                reasoning_started = True
            reasoning_content += delta.reasoning_content
            print(delta.reasoning_content, end="", flush=True)

        # Handle streaming response content output
        if hasattr(delta, 'content') and delta.content:
            if not content_started and delta.content.strip():
                print("\n\nðŸ’¬ Response Content:")
                content_started = True
            content += delta.content
            print(delta.content, end="", flush=True)

        # Handle streaming tool call information
        if delta.tool_calls:
            for tool_call in delta.tool_calls:
                index = tool_call.index
                if index not in final_tool_calls:
                    # New tool call
                    final_tool_calls[index] = tool_call
                    final_tool_calls[index].function.arguments = tool_call.function.arguments
                else:
                    # Append tool call parameters (streaming construction)
                    final_tool_calls[index].function.arguments += tool_call.function.arguments

    # Output final tool call information
    if final_tool_calls:
        print("\nðŸ“‹ Function Calls Triggered:")
        for index, tool_call in final_tool_calls.items():
            print(f"  {index}: Function Name: {tool_call.function.name}, Parameters: {tool_call.function.arguments}")
    ```
  </Tab>
</Tabs>

## Application Scenarios

<CardGroup cols={2}>
  <Card title="Intelligent Customer Service" icon="headset">
    * Real-time query progress display
    * Improved waiting experience
  </Card>

  <Card title="Code Assistant" icon="code">
    * Real-time code analysis process
    * Display tool call chains
  </Card>
</CardGroup>


---

# Function Calling

<Tip>
  Function Calling allows AI models to call external functions and APIs, greatly expanding the capability boundaries of intelligent agents, enabling them to perform specific operations and obtain real-time data.
</Tip>

## Features

Function calling provides AI models with the ability to interact with external systems, supporting various complex application scenarios and integration requirements.

### Core Parameter Description

* **`tools`**: Defines the list of callable functions, including function names, descriptions, and parameter specifications
* **`tool_choice`**: Controls function calling strategy, default is `auto` (only supports `auto`)
* **`model`**: Uses models that support function calling, such as `glm-4-plus`, `glm-4.6`, etc.

### Response Parameter Description

Key fields in function calling responses:

* **`tool_calls`**: Contains information about functions the model decides to call
* **`function.name`**: Name of the called function
* **`function.arguments`**: Function call parameters (JSON format string)
* **`id`**: Unique identifier for the tool call

## Code Examples

By defining function tools and handling function calls, AI models can perform various external operations:

<Tabs>
  <Tab title="Python SDK">
    **Install SDK**

    ```bash  theme={null}
    # Install latest version
    pip install zai-sdk

    # Or specify version
    pip install zai-sdk==0.1.0
    ```

    **Verify Installation**

    ```python  theme={null}
    import zai
    print(zai.__version__)
    ```

    **Complete Example**

    ```python  theme={null}
    import json
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='your_api_key')

    # Define weather query function
    def get_weather(city: str) -> dict:
        """Get weather information for specified city"""
        # This should call a real weather API
        weather_data = {
            "city": city,
            "temperature": "22Â°C",
            "condition": "Sunny",
            "humidity": "65%",
            "wind_speed": "5 km/h"
        }
        return weather_data

    # Define function tools
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather information for specified city",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "City name, e.g.: Beijing, Shanghai"
                        }
                    },
                    "required": ["city"]
                }
            }
        }
    ]

    # Make conversation request
    response = client.chat.completions.create(
        model="glm-4.7",  # Use model that supports function calling
        messages=[
            {"role": "user", "content": "How's the weather in Beijing today?"}
        ],
        tools=tools,         # Pass function tools
        tool_choice="auto"   # Automatically choose whether to call functions
    )

    # Handle function calls
    message = response.choices[0].message
    messages = [{"role": "user", "content": "How's the weather in Beijing today?"}]
    messages.append(message.model_dump())

    if message.tool_calls:
        for tool_call in message.tool_calls:
            if tool_call.function.name == "get_weather":
                # Parse parameters and call function
                args = json.loads(tool_call.function.arguments)
                weather_result = get_weather(args.get("city"))
                
                # Return function result to model
                messages.append({
                    "role": "tool",
                    "content": json.dumps(weather_result, ensure_ascii=False),
                    "tool_call_id": tool_call.id
                })
        
        # Get final answer
        final_response = client.chat.completions.create(
            model="glm-4.7",
            messages=messages,
            tools=tools
        )
        
        print(final_response.choices[0].message.content)
    else:
        print(message.content)
    ```
  </Tab>
</Tabs>

## Scenario Examples

<Warning>
  When using function calling, please ensure proper security validation and permission control for external APIs and database operations.
</Warning>

<Accordion title="Multi-function Assistant">
  ```python  theme={null}
  import json
  import requests
  from datetime import datetime
  from zai import ZaiClient

  class FunctionAgent:
      def __init__(self, api_key):
          self.client = ZaiClient(api_key=api_key)
          self.tools = self._define_tools()
      
      def _define_tools(self):
          return [
              {
                  "type": "function",
                  "function": {
                      "name": "get_current_time",
                      "description": "Get current time",
                      "parameters": {
                          "type": "object",
                          "properties": {},
                          "required": []
                      }
                  }
              },
              {
                  "type": "function",
                  "function": {
                      "name": "calculate",
                      "description": "Perform mathematical calculations",
                      "parameters": {
                          "type": "object",
                          "properties": {
                              "expression": {
                                  "type": "string",
                                  "description": "Mathematical expression, e.g.: 2+3*4"
                              }
                          },
                          "required": ["expression"]
                      }
                  }
              },
              {
                  "type": "function",
                  "function": {
                      "name": "search_web",
                      "description": "Search web information",
                      "parameters": {
                          "type": "object",
                          "properties": {
                              "query": {
                                  "type": "string",
                                  "description": "Search keywords"
                              }
                          },
                          "required": ["query"]
                      }
                  }
              }
          ]
      
      def get_current_time(self):
          """Get current time"""
          return {
              "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
              "timezone": "Asia/Shanghai"
          }
      
      def calculate(self, expression: str):
          """Safe mathematical calculation"""
          try:
              # Simple security check
              allowed_chars = set('0123456789+-*/().')
              if not all(c in allowed_chars or c.isspace() for c in expression):
                  return {"error": "Expression contains disallowed characters"}
              
              result = eval(expression)
              return {
                  "expression": expression,
                  "result": result
              }
          except Exception as e:
              return {"error": f"Calculation error: {str(e)}"}
      
      def search_web(self, query: str):
          """Simulate web search"""
          # This should call a real search API
          return {
              "query": query,
              "results": [
                  {"title": f"Search result 1 about {query}", "url": "https://example1.com"},
                  {"title": f"Search result 2 about {query}", "url": "https://example2.com"}
              ]
          }
      
      def execute_function(self, function_name: str, arguments: dict):
          """Execute function call"""
          if function_name == "get_current_time":
              return self.get_current_time()
          elif function_name == "calculate":
              return self.calculate(arguments.get("expression", ""))
          elif function_name == "search_web":
              return self.search_web(arguments.get("query", ""))
          else:
              return {"error": f"Unknown function: {function_name}"}
      
      def chat(self, user_message: str):
          """Handle user message"""
          messages = [{"role": "user", "content": user_message}]
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=messages,
              tools=self.tools,
              tool_choice="auto"
          )
          
          message = response.choices[0].message
          messages.append(message.model_dump())
          
          # Handle function calls
          if message.tool_calls:
              for tool_call in message.tool_calls:
                  function_name = tool_call.function.name
                  arguments = json.loads(tool_call.function.arguments)
                  
                  # Execute function
                  result = self.execute_function(function_name, arguments)
                  
                  # Add function result
                  messages.append({
                      "role": "tool",
                      "content": json.dumps(result, ensure_ascii=False),
                      "tool_call_id": tool_call.id
                  })
              
              # Get final answer
              final_response = self.client.chat.completions.create(
                  model="glm-4.7",
                  messages=messages,
                  tools=self.tools
              )
              
              return final_response.choices[0].message.content
          else:
              return message.content

  # Usage example
  agent = FunctionAgent("your_api_key")

  # Test different types of requests
  print(agent.chat("What time is it now?"))
  print(agent.chat("Help me calculate 15 * 23 + 7"))
  print(agent.chat("Search for the latest developments in artificial intelligence"))
  ```
</Accordion>

<Accordion title="Database Query">
  ```python  theme={null}
  import sqlite3

  def query_database(sql: str) -> dict:
      """Execute database query"""
      try:
          conn = sqlite3.connect('example.db')
          cursor = conn.cursor()
          cursor.execute(sql)
          results = cursor.fetchall()
          conn.close()
          
          return {
              "success": True,
              "data": results,
              "row_count": len(results)
          }
      except Exception as e:
          return {
              "success": False,
              "error": str(e)
          }

  # Function definition
  db_tool = {
      "type": "function",
      "function": {
          "name": "query_database",
          "description": "Execute SQL query",
          "parameters": {
              "type": "object",
              "properties": {
                  "sql": {
                      "type": "string",
                      "description": "SQL query statement"
                  }
              },
              "required": ["sql"]
          }
      }
  }
  ```
</Accordion>

<Accordion title="File Operations">
  ```python  theme={null}
  import os
  import json

  def file_operations(operation: str, file_path: str, content: str = None) -> dict:
      """File operation function"""
      try:
          if operation == "read":
              with open(file_path, 'r', encoding='utf-8') as f:
                  content = f.read()
              return {"success": True, "content": content}
          
          elif operation == "write":
              with open(file_path, 'w', encoding='utf-8') as f:
                  f.write(content)
              return {"success": True, "message": "File written successfully"}
          
          elif operation == "list":
              files = os.listdir(file_path)
              return {"success": True, "files": files}
          
          else:
              return {"success": False, "error": "Unsupported operation"}
      
      except Exception as e:
          return {"success": False, "error": str(e)}

  # Function definition
  file_tool = {
      "type": "function",
      "function": {
          "name": "file_operations",
          "description": "Execute file operations",
          "parameters": {
              "type": "object",
              "properties": {
                  "operation": {
                      "type": "string",
                      "enum": ["read", "write", "list"],
                      "description": "Operation type"
                  },
                  "file_path": {
                      "type": "string",
                      "description": "File path"
                  },
                  "content": {
                      "type": "string",
                      "description": "Content to write (only required for write operation)"
                  }
              },
              "required": ["operation", "file_path"]
          }
      }
  }
  ```
</Accordion>

<Accordion title="API Integration">
  ```python  theme={null}
  import requests

  def call_external_api(url: str, method: str = "GET", headers: dict = None, data: dict = None) -> dict:
      """Call external API"""
      try:
          if method.upper() == "GET":
              response = requests.get(url, headers=headers, params=data)
          elif method.upper() == "POST":
              response = requests.post(url, headers=headers, json=data)
          else:
              return {"success": False, "error": "Unsupported HTTP method"}
          
          return {
              "success": True,
              "status_code": response.status_code,
              "data": response.json() if response.headers.get('content-type', '').startswith('application/json') else response.text
          }
      
      except Exception as e:
          return {"success": False, "error": str(e)}

  # Function definition
  api_tool = {
      "type": "function",
      "function": {
          "name": "call_external_api",
          "description": "Call external API",
          "parameters": {
              "type": "object",
              "properties": {
                  "url": {
                      "type": "string",
                      "description": "API endpoint URL"
                  },
                  "method": {
                      "type": "string",
                      "enum": ["GET", "POST"],
                      "description": "HTTP method"
                  },
                  "headers": {
                      "type": "object",
                      "description": "Request headers"
                  },
                  "data": {
                      "type": "object",
                      "description": "Request data"
                  }
              },
              "required": ["url"]
          }
      }
  }
  ```
</Accordion>

## Best Practices

<CardGroup cols={2}>
  <Card title="Function Design Principles" icon="code">
    * Single responsibility: Each function should do one thing
    * Clear naming: Function and parameter names should be meaningful
    * Complete description: Provide detailed function and parameter descriptions
  </Card>

  <Card title="Security Considerations" icon="shield">
    * Input validation: Strictly validate all input parameters
    * Permission control: Limit function access permissions
    * Logging: Record function call logs
  </Card>
</CardGroup>

### Parameter Design

```python  theme={null}
# Good parameter design
{
    "type": "object",
    "properties": {
        "city": {
            "type": "string",
            "description": "City name, supports Chinese and English, e.g.: Beijing, Shanghai, New York",
            "examples": ["Beijing", "Shanghai", "New York"]
        },
        "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit",
            "default": "celsius"
        }
    },
    "required": ["city"]
}
```

### Error Handling

```python  theme={null}
def robust_function(param: str) -> dict:
    """Robust function implementation"""
    try:
        # Parameter validation
        if not param or not isinstance(param, str):
            return {
                "success": False,
                "error": "Invalid parameter",
                "error_code": "INVALID_PARAM"
            }
        
        # Business logic
        result = process_data(param)
        
        return {
            "success": True,
            "data": result,
            "timestamp": datetime.now().isoformat()
        }
    
    except ValueError as e:
        return {
            "success": False,
            "error": f"Data error: {str(e)}",
            "error_code": "DATA_ERROR"
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"System error: {str(e)}",
            "error_code": "SYSTEM_ERROR"
        }
```

### Input Validation

```python  theme={null}
def secure_function(user_input: str) -> dict:
    """Secure function implementation"""
    # Input length limit
    if len(user_input) > 1000:
        return {"error": "Input too long"}
    
    # Dangerous character filtering
    dangerous_chars = ['<', '>', '&', '"', "'"]
    if any(char in user_input for char in dangerous_chars):
        return {"error": "Input contains dangerous characters"}
    
    # SQL injection protection
    sql_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT']
    if any(keyword in user_input.upper() for keyword in sql_keywords):
        return {"error": "Input contains dangerous keywords"}
    
    return {"success": True, "processed_input": user_input}
```

### Permission Control

```python  theme={null}
def check_permissions(user_id: str, operation: str) -> bool:
    """Check user permissions"""
    user_permissions = get_user_permissions(user_id)
    return operation in user_permissions

def protected_function(user_id: str, operation: str, data: dict) -> dict:
    """Function requiring permission validation"""
    if not check_permissions(user_id, operation):
        return {
            "success": False,
            "error": "Insufficient permissions",
            "error_code": "PERMISSION_DENIED"
        }
    
    # Execute operation
    return perform_operation(operation, data)
```

<Tip>
  It is recommended to provide detailed documentation and examples for each function to help the model better understand the function's purpose and usage.
</Tip>

<Warning>
  Function calling involves code execution. Please ensure appropriate security measures are implemented, including input validation, permission control, and error handling.
</Warning>


---

# Context Caching

<Tip>
  Context caching functionality significantly reduces token consumption and response latency by caching repeated context content. When you repeatedly use the same system prompts or conversation history in dialogues, the caching mechanism automatically identifies and reuses this content, thereby improving performance and reducing costs.
</Tip>

## Features

* **Automatic Cache Recognition**: Implicit caching that intelligently identifies repeated context content without manual configuration
* **Significant Cost Reduction**: Cached tokens are billed at lower prices, dramatically saving costs
* **Improved Response Speed**: Reduces processing time for repeated content, accelerating model responses
* **Transparent Billing**: Detailed display of cached token counts in response field `usage.prompt_tokens_details.cached_tokens`
* **Wide Compatibility**: Supports all mainstream models, including GLM-4.6, GLM-4.5 series, etc.

> Context caching works by computing input message content and identifying content that is identical or highly similar to previous requests. When repeated content is detected, the system reuses previous computation results, avoiding redundant token processing.

This mechanism is particularly suitable for the following scenarios:

* System prompt reuse: In multi-turn conversations, system prompts usually remain unchanged, and caching can significantly reduce token consumption for this part.
* Repetitive tasks: For tasks that process similar content with consistent instructions multiple times, caching can improve efficiency.
* Multi-turn conversation history: In complex conversations, historical messages often contain a lot of repeated information, and caching can effectively reduce token usage for this part.

## Code Examples

<Tabs>
  <Tab title="cURL">
    **Basic Caching Example**

    ```bash  theme={null}
    # First request - establish cache
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "system",
                "content": "You are a professional data analyst, skilled at explaining data trends and providing business insights."
            },
            {
                "role": "user",
                "content": "How to analyze user retention rate?"
            }
        ]
    }'
    ```

    **Cache Reuse Example**

    ```bash  theme={null}
    # Second request - reuse system prompt cache
    curl --location 'https://api.z.ai/api/paas/v4/chat/completions' \
    --header 'Authorization: Bearer YOUR_API_KEY' \
    --header 'Content-Type: application/json' \
    --data '{
        "model": "glm-4.7",
        "messages": [
            {
                "role": "system",
                "content": "You are a professional data analyst, skilled at explaining data trends and providing business insights."
            },
            {
                "role": "user",
                "content": "What is funnel analysis?"
            }
        ]
    }'
    ```
  </Tab>

  <Tab title="Python SDK">
    **Install SDK**

    ```bash  theme={null}
    # Install latest version
    pip install zai-sdk

    # Or specify version
    pip install zai-sdk==0.1.0
    ```

    **Verify Installation**

    ```python  theme={null}
    import zai
    print(zai.__version__)
    ```

    **Basic Conversation Example**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='Your API key')

    # First request - establish cache
    response1 = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "system",
                "content": "You are a professional technical documentation assistant, skilled at explaining complex technical concepts. Please answer user questions with clear and concise language, and provide practical code examples."
            },
            {
                "role": "user",
                "content": "What is RESTful API?"
            }
        ]
    )

    print("First request result:")
    print(f"Reply: {response1.choices[0].message.content}")
    print(f"Total tokens: {response1.usage.total_tokens}")
    print(f"Cached tokens: {response1.usage.prompt_tokens_details.cached_tokens if hasattr(response1.usage, 'prompt_tokens_details') else 0}")

    # Second request - reuse system prompt cache
    response2 = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "system",
                "content": "You are a professional technical documentation assistant, skilled at explaining complex technical concepts. Please answer user questions with clear and concise language, and provide practical code examples."  # Same system prompt
            },
            {
                "role": "user",
                "content": "What are the differences between GraphQL and RESTful API?"
            }
        ]
    )

    print("\nSecond request result:")
    print(f"Reply: {response2.choices[0].message.content}")
    print(f"Total tokens: {response2.usage.total_tokens}")
    print(f"Cached tokens: {response2.usage.prompt_tokens_details.cached_tokens if hasattr(response2.usage, 'prompt_tokens_details') else 0}")
    ```

    **Long Document Analysis Example**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='Your API key')

    # Long document content (simulated)
    long_document = """
    This is a detailed technical specification document that includes system architecture, API design, database structure, and many other aspects.
    The document is very long and contains a lot of technical details and implementation instructions...
    [Large amount of document content omitted here]
    """

    # First analysis - establish document cache
    response1 = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "system",
                "content": f"Please answer user questions based on the following technical document:\n\n{long_document}"
            },
            {
                "role": "user",
                "content": "What is the main architecture of this system?"
            }
        ]
    )

    print("First analysis:")
    print(f"Total tokens: {response1.usage.total_tokens}")
    print(f"Cached tokens: {response1.usage.prompt_tokens_details.cached_tokens if hasattr(response1.usage, 'prompt_tokens_details') else 0}")

    # Second analysis - reuse document cache
    response2 = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "system",
                "content": f"Please answer user questions based on the following technical document:\n\n{long_document}"  # Same document content
            },
            {
                "role": "user",
                "content": "What are the characteristics of the API design?"
            }
        ]
    )

    print("\nSecond analysis:")
    print(f"Total tokens: {response2.usage.total_tokens}")
    print(f"Cached tokens: {response2.usage.prompt_tokens_details.cached_tokens if hasattr(response2.usage, 'prompt_tokens_details') else 0}")
    print(f"Cache savings: {response2.usage.prompt_tokens_details.cached_tokens / response2.usage.total_tokens * 100:.1f}%")
    ```

    **Multi-turn Conversation Caching Example**

    ```python  theme={null}
    from zai import ZaiClient

    # Initialize client
    client = ZaiClient(api_key='Your API key')

    # Build conversation history
    conversation_history = [
        {"role": "system", "content": "You are a Python programming assistant, helping users solve programming problems."},
        {"role": "user", "content": "How to create a simple Flask application?"},
        {"role": "assistant", "content": "Creating a Flask application is simple, first install Flask..."},
        {"role": "user", "content": "How to add routes?"},
        {"role": "assistant", "content": "In Flask, add routes using the @app.route decorator..."},
    ]

    # Continue conversation - reuse conversation history cache
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=conversation_history + [
            {"role": "user", "content": "How to handle POST requests?"}
        ]
    )

    print("Conversation reply:")
    print(f"Content: {response.choices[0].message.content}")
    print(f"Total tokens: {response.usage.total_tokens}")
    print(f"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens if hasattr(response.usage, 'prompt_tokens_details') else 0}")

    # Calculate cache efficiency
    if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details.cached_tokens:
        cache_ratio = response.usage.prompt_tokens_details.cached_tokens / response.usage.prompt_tokens * 100
        print(f"Cache hit rate: {cache_ratio:.1f}%")
    ```

    **Batch Processing Optimization Example**

    ````python  theme={null}
    from zai import ZaiClient
    import time

    # Initialize client
    client = ZaiClient(api_key='Your API key')

    # Common system prompt
    system_prompt = """
    You are a professional code review assistant. Please analyze the provided code from the following aspects:
    1. Code quality and readability
    2. Performance optimization suggestions
    3. Security considerations
    4. Best practice recommendations
    Please provide specific improvement suggestions.
    """

    # List of code snippets to review
    code_snippets = [
        "def calculate_sum(numbers): return sum(numbers)",
        "class User: def __init__(self, name): self.name = name",
        "for i in range(len(items)): print(items[i])",
        "if user_input == 'yes' or user_input == 'y': return True"
    ]

    results = []
    total_cached_tokens = 0

    for i, code in enumerate(code_snippets):
        start_time = time.time()
        
        response = client.chat.completions.create(
            model="glm-4.7",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Please review the following code:\n```python\n{code}\n```"}
            ]
        )
        
        end_time = time.time()
        
        # Count cache effects
        cached_tokens = 0
        if hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details.cached_tokens:
            cached_tokens = response.usage.prompt_tokens_details.cached_tokens
            total_cached_tokens += cached_tokens
        
        results.append({
            'code': code,
            'review': response.choices[0].message.content,
            'total_tokens': response.usage.total_tokens,
            'cached_tokens': cached_tokens,
            'response_time': end_time - start_time
        })
        
        print(f"Code snippet {i+1} review completed:")
        print(f"  Response time: {end_time - start_time:.2f}s")
        print(f"  Cached tokens: {cached_tokens}")
        print(f"  Total tokens: {response.usage.total_tokens}")
        print()

    print(f"Batch processing completed, total cached tokens: {total_cached_tokens}")
    ````
  </Tab>
</Tabs>

Response contains context cache token usage information:

```json  theme={null}
{
  "usage": {
    "prompt_tokens": 1200,
    "completion_tokens": 300,
    "total_tokens": 1500,
    "prompt_tokens_details": {
      "cached_tokens": 800
    }
  }
}
```

## Best Practices

<Tabs>
  <Tab title="System Prompt Optimization">
    Use stable system prompts

    ```python  theme={null}
    # Recommended: Use stable system prompts
    system_prompt = """
    You are a professional technical consultant with the following characteristics:
    - Deep technical background and rich project experience
    - Able to provide accurate and practical technical advice
    - Good at explaining complex concepts in clear and concise language
    Please provide professional technical guidance based on user questions.
    """
    ```
  </Tab>

  <Tab title="Document Content Reuse">
    Use long documents as system messages

    ```python  theme={null}
    # Recommended: Use long documents as system messages
    def create_document_based_chat(document_content, user_question):
        return client.chat.completions.create(
            model="glm-4.7",
            messages=[
                {
                    "role": "system",
                    "content": f"Please answer user questions based on the following document content:\n\n{document_content}"
                },
                {
                    "role": "user",
                    "content": user_question
                }
            ]
        )

    # Multiple calls with the same document, system prompts will be cached
    questions = ["What is the main content of the document?", "What are the key points?", "How to implement these suggestions?"]
    for question in questions:
        response = create_document_based_chat(document_content, question)
        # Second and subsequent calls will hit the cache
    ```
  </Tab>

  <Tab title="Conversation History Management">
    Manage conversation history to improve cache efficiency

    ```python  theme={null}
    class ConversationManager:
        def __init__(self, client, system_prompt):
            self.client = client
            self.system_prompt = system_prompt
            self.history = [{"role": "system", "content": system_prompt}]
        
        def add_message(self, role, content):
            self.history.append({"role": role, "content": content})
        
        def get_response(self, user_message):
            # Add user message
            self.add_message("user", user_message)
            
            # Get reply (conversation history will be cached)
            response = self.client.chat.completions.create(
                model="glm-4.7",
                messages=self.history
            )
            
            # Add assistant reply to history
            assistant_message = response.choices[0].message.content
            self.add_message("assistant", assistant_message)
            
            return response
        
        def get_cache_stats(self, response):
            """Get cache statistics"""
            if hasattr(response.usage, 'prompt_tokens_details'):
                cached = response.usage.prompt_tokens_details.cached_tokens or 0
                total = response.usage.prompt_tokens
                return f"Cache hit: {cached}/{total} ({cached/total*100:.1f}%)"
            return "No cache information"

    # Usage example
    manager = ConversationManager(client, "You are a programming assistant...")
    response1 = manager.get_response("How to learn Python?")
    response2 = manager.get_response("Recommend some learning resources")  # Will reuse previous conversation cache
    ```
  </Tab>
</Tabs>

## Use Cases

<CardGroup cols={2}>
  <Card title="Multi-turn Conversations" icon="headset">
    * Intelligent customer service systems
    * Personal assistant services
  </Card>

  <Card title="Batch Processing" icon="cubes">
    * Code review batch processing
    * Content batch analysis
  </Card>

  <Card title="Template Applications" icon="rectangle-list">
    * Report generation templates
    * Standardized process handling
  </Card>

  <Card title="Education and Training" icon="glasses">
    * Homework grading assistance
    * Learning material analysis
  </Card>
</CardGroup>

## Important Notes

<Tabs>
  <Tab title="Understanding Cache Mechanism">
    * Caching is automatically triggered based on content similarity, no manual configuration required
    * Identical content has the highest cache hit rate
    * Minor formatting differences may affect cache effectiveness
    * Cache has reasonable time limits, will recalculate after expiration
  </Tab>

  <Tab title="Cost Optimization Suggestions">
    * Cached tokens are billed at lower prices
    * Long documents and repeated content have the most significant cache effects
    * Design system prompts reasonably to improve reuse rates
    * Monitor cache hit rates and optimize usage patterns
  </Tab>

  <Tab title="Performance Considerations">
    * Caching can significantly improve response speed
    * First request to establish cache may be slightly slower
    * Manage conversation history length reasonably
    * Avoid overly frequent content changes
  </Tab>

  <Tab title="Best Practices">
    * Use stable system prompt templates
    * Process long documents as system messages
    * Organize conversation history structure reasonably
    * Regularly analyze cache effectiveness and optimize
  </Tab>
</Tabs>

## Billing Information

Context caching uses a differentiated billing strategy:

* New content tokens: Billed at standard prices
* Cache hit tokens: Billed at discounted prices (usually 50% of standard price)
* Output tokens: Billed at standard prices

Billing example:

```
Assuming standard price is 0.01 /1K tokens:

Request details:
- Total input tokens: 2000
- Cache hit tokens: 1200
- New content tokens: 800
- Output tokens: 500

Billing calculation:
- New content cost: 800 Ã— 0.01/1000 = 0.008
- Cache cost: 1200 Ã— 0.005/1000 = 0.006
- Output cost: 500 Ã— 0.01/1000 = 0.005
- Total cost: 0.019

Compared to no cache (2500 Ã— 0.01/1000 = 0.025), saves 24%
```


---

# Structured Output

<Tip>
  Structured output (JSON mode) ensures that AI returns JSON data conforming to predefined formats, providing reliable guarantees for programmatic processing of AI outputs.
</Tip>

## Features

The structured output feature provides AI models with strict data format control capabilities, supporting various complex data structures and validation requirements.

### Core Parameters

* **`response_format`**: Specifies the response format, set to `{"type": "json_object"}` to enable JSON mode
* **`model`**: Use models that support structured output, such as `glm-4.5`, `glm-4.6`, etc.
* **`messages`**: Define the expected JSON structure and field requirements in system messages

## Code Examples

**Install SDK**

```bash  theme={null}
# Install latest version
pip install zai-sdk

# Or specify version
pip install zai-sdk==0.1.0
```

**Verify Installation**

```python  theme={null}
import zai
print(zai.__version__)
```

**Complete Example**

The following is a complete structured output example demonstrating how to perform sentiment analysis and return structured JSON results:

```python  theme={null}
from zai import ZaiClient
import json

# Initialize client
client = ZaiClient(api_key="your-api-key")

# Basic JSON mode
response = client.chat.completions.create(
    model="glm-4.7",
    messages=[
        {
            "role": "system",
            "content": """
            You are a sentiment analysis expert. Please return analysis results in the following JSON format:
            {
                "sentiment": "positive/negative/neutral",
                "confidence": 0.95,
                "emotions": ["joy", "excitement"],
                "keywords": ["weather", "mood"],
                "analysis": "Detailed analysis explanation"
            }
            """
        },
        {
            "role": "user",
            "content": "Please analyze the sentiment of this sentence: 'The weather is really nice today, I'm feeling very happy!'"
        }
    ],
    response_format={
        "type": "json_object"
    }
)

# Parse results
result = json.loads(response.choices[0].message.content)
print(f"Sentiment: {result['sentiment']}")
print(f"Confidence: {result['confidence']}")
print(f"Emotions: {result['emotions']}")
```

## Basic Usage

<Tabs>
  <Tab title="Simple JSON Output">
    **Simple JSON Output**

    ```python  theme={null}
    from zai import ZaiClient

    client = ZaiClient(api_key="your-api-key")

    # Basic JSON mode
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "user",
                "content": "Please analyze the sentiment of this sentence: 'The weather is really nice today, I'm feeling very happy!'"
            }
        ],
        response_format={
            "type": "json_object"
        }
    )

    import json
    result = json.loads(response.choices[0].message.content)
    print(result)
    ```
  </Tab>

  <Tab title="Specify JSON Structure">
    ### Specify JSON Structure

    ```python  theme={null}
    # Specify specific JSON structure
    response = client.chat.completions.create(
        model="glm-4.7",
        messages=[
            {
                "role": "system",
                "content": """
                You are a sentiment analysis expert. Please return analysis results in the following JSON format:
                {
                    "sentiment": "positive/negative/neutral",
                    "confidence": 0.95,
                    "emotions": ["joy", "excitement"],
                    "keywords": ["weather", "mood"],
                    "analysis": "Detailed analysis explanation"
                }
                """
            },
            {
                "role": "user",
                "content": "Please analyze the sentiment of this sentence: 'The weather is really nice today, I'm feeling very happy!'"
            }
        ],
        response_format={
            "type": "json_object"
        }
    )

    result = json.loads(response.choices[0].message.content)
    print(f"Sentiment: {result['sentiment']}")
    print(f"Confidence: {result['confidence']}")
    print(f"Emotions: {result['emotions']}")
    ```
  </Tab>

  <Tab title="Schema Validation">
    ### Using JSON Schema Validation

    ```python  theme={null}
    import jsonschema
    from jsonschema import validate

    # Define JSON Schema
    schema = {
        "type": "object",
        "properties": {
            "sentiment": {
                "type": "string",
                "enum": ["positive", "negative", "neutral"]
            },
            "confidence": {
                "type": "number",
                "minimum": 0,
                "maximum": 1
            },
            "emotions": {
                "type": "array",
                "items": {"type": "string"}
            },
            "keywords": {
                "type": "array",
                "items": {"type": "string"}
            },
            "analysis": {
                "type": "string"
            }
        },
        "required": ["sentiment", "confidence", "analysis"]
    }

    def analyze_sentiment_with_validation(text):
        """Sentiment analysis with validation"""
        response = client.chat.completions.create(
            model="glm-4.7",
            messages=[
                {
                    "role": "system",
                    "content": f"""
                    Please return sentiment analysis results according to the following JSON Schema format:
                    {json.dumps(schema, indent=2, ensure_ascii=False)}
                    """
                },
                {
                    "role": "user",
                    "content": f"Please analyze the sentiment of this sentence: '{text}'"
                }
            ],
            response_format={"type": "json_object"}
        )
        
        try:
            result = json.loads(response.choices[0].message.content)
            # Validate JSON structure
            validate(instance=result, schema=schema)
            return result
        except jsonschema.exceptions.ValidationError as e:
            print(f"JSON validation failed: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"JSON parsing failed: {e}")
            return None

    # Usage example
    result = analyze_sentiment_with_validation("The weather is really nice today, I'm feeling very happy!")
    if result:
        print("Analysis result:", result)
    ```
  </Tab>
</Tabs>

## Scenario Examples

<Warning>
  When using JSON mode for data extraction, please ensure the quality and format of input data to achieve the best extraction results.
</Warning>

<Accordion title="Data Extraction and Structuring Complete Implementation">
  ```python  theme={null}
  class DataExtractor:
      def __init__(self, api_key):
          self.client = ZaiClient(api_key=api_key)
      
      def extract_contact_info(self, text):
          """Extract contact information"""
          schema = {
              "type": "object",
              "properties": {
                  "contacts": {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "properties": {
                              "name": {"type": "string"},
                              "phone": {"type": "string"},
                              "email": {"type": "string"},
                              "company": {"type": "string"},
                              "position": {"type": "string"},
                              "address": {"type": "string"}
                          },
                          "required": ["name"]
                      }
                  },
                  "total_count": {"type": "integer"},
                  "extraction_confidence": {"type": "number"}
              },
              "required": ["contacts", "total_count"]
          }
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      You are an information extraction expert. Please extract all contact information from the text,
                      return in the following JSON format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Note:
                      - If a field has no information, do not include that field
                      - phone field should be in standardized phone number format
                      - email field should be a valid email address
                      - extraction_confidence represents overall extraction confidence (0-1)
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Please extract contact information from the following text:\n\n{text}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)["properties"]
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Extraction failed: {e}")
              return None
      
      def extract_product_info(self, product_description):
          """Extract product information"""
          schema = {
              "type": "object",
              "properties": {
                  "product_name": {"type": "string"},
                  "brand": {"type": "string"},
                  "category": {"type": "string"},
                  "price": {
                      "type": "object",
                      "properties": {
                          "amount": {"type": "number"},
                          "currency": {"type": "string"},
                          "original_price": {"type": "number"},
                          "discount": {"type": "number"}
                      }
                  },
                  "specifications": {
                      "type": "object",
                      "additionalProperties": True
                  },
                  "features": {
                      "type": "array",
                      "items": {"type": "string"}
                  },
                  "availability": {
                      "type": "object",
                      "properties": {
                          "in_stock": {"type": "boolean"},
                          "quantity": {"type": "integer"},
                          "shipping_time": {"type": "string"}
                      }
                  },
                  "ratings": {
                      "type": "object",
                      "properties": {
                          "average_rating": {"type": "number"},
                          "total_reviews": {"type": "integer"}
                      }
                  }
              },
              "required": ["product_name"]
          }
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please extract structured information from product description, return in the following format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Note:
                      - Price information should accurately extract values and currency units
                      - specifications should include all technical specifications
                      - features should list main functional features
                      - Do not guess if information is unclear
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Product description:\n{product_description}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Product information extraction failed: {e}")
              return None
      
      def extract_event_info(self, event_text):
          """Extract event information"""
          schema = {
              "type": "object",
              "properties": {
                  "events": {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "properties": {
                              "title": {"type": "string"},
                              "description": {"type": "string"},
                              "start_time": {"type": "string"},
                              "end_time": {"type": "string"},
                              "location": {"type": "string"},
                              "organizer": {"type": "string"},
                              "participants": {
                                  "type": "array",
                                  "items": {"type": "string"}
                              },
                              "category": {"type": "string"},
                              "priority": {
                                  "type": "string",
                                  "enum": ["high", "medium", "low"]
                              },
                              "status": {
                                  "type": "string",
                                  "enum": ["scheduled", "ongoing", "completed", "cancelled"]
                              }
                          },
                          "required": ["title", "start_time"]
                      }
                  }
              },
              "required": ["events"]
          }
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please extract all event information from the text, return in the following format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Time format requirements:
                      - Use ISO 8601 format: YYYY-MM-DDTHH:MM:SS
                      - If only date available, use: YYYY-MM-DD
                      - If time is unclear, try to infer reasonable time
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Please extract event information from the following text:\n\n{event_text}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Event information extraction failed: {e}")
              return None

  # Usage example
  extractor = DataExtractor("your_api_key")

  # Extract contact information
  contact_text = """
  Zhang San, mobile: 13800138000, email: zhangsan@example.com,
  works as Technical Director at Beijing Technology Co., Ltd.
  Company address: No. 123, Technology Park, Chaoyang District, Beijing.

  Li Si, phone: 010-12345678, work email: lisi@company.com,
  is a Product Manager at Shanghai Innovation Company.
  """

  contacts = extractor.extract_contact_info(contact_text)
  if contacts:
      print(f"Extracted {contacts['total_count']} contacts")
      for contact in contacts['contacts']:
          print(f"Name: {contact['name']}")
          if 'phone' in contact:
              print(f"Phone: {contact['phone']}")
  ```
</Accordion>

<Accordion title="API Response Formatting Complete Implementation">
  ```python  theme={null}
  class APIResponseFormatter:
      def __init__(self, api_key):
          self.client = ZaiClient(api_key=api_key)
      
      def format_search_results(self, query, raw_results):
          """Format search results"""
          schema = {
              "type": "object",
              "properties": {
                  "query": {"type": "string"},
                  "total_results": {"type": "integer"},
                  "results": {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "properties": {
                              "title": {"type": "string"},
                              "url": {"type": "string"},
                              "snippet": {"type": "string"},
                              "relevance_score": {"type": "number"},
                              "source_type": {"type": "string"},
                              "publish_date": {"type": "string"},
                              "tags": {
                                  "type": "array",
                                  "items": {"type": "string"}
                              }
                          },
                          "required": ["title", "url", "snippet"]
                      }
                  },
                  "suggestions": {
                      "type": "array",
                      "items": {"type": "string"}
                  },
                  "filters": {
                      "type": "object",
                      "properties": {
                          "date_range": {"type": "string"},
                          "source_types": {
                              "type": "array",
                              "items": {"type": "string"}
                          },
                          "languages": {
                              "type": "array",
                              "items": {"type": "string"}
                          }
                      }
                  }
              },
              "required": ["query", "total_results", "results"]
          }
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please format search results into standard JSON format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Requirements:
                      - Calculate relevance score for each result (0-1)
                      - Identify content type (article, video, image, document, etc.)
                      - Extract publish date (if available)
                      - Generate relevant tags
                      - Provide search suggestions
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Query: {query}\n\nRaw results:\n{raw_results}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Formatting failed: {e}")
              return None
      
      def format_analytics_data(self, raw_data, metrics):
          """Format analytics data"""
          schema = {
              "type": "object",
              "properties": {
                  "summary": {
                      "type": "object",
                      "properties": {
                          "total_records": {"type": "integer"},
                          "date_range": {
                              "type": "object",
                              "properties": {
                                  "start_date": {"type": "string"},
                                  "end_date": {"type": "string"}
                              }
                          },
                          "key_insights": {
                              "type": "array",
                              "items": {"type": "string"}
                          }
                      }
                  },
                  "metrics": {
                      "type": "object",
                      "additionalProperties": {
                          "type": "object",
                          "properties": {
                              "current_value": {"type": "number"},
                              "previous_value": {"type": "number"},
                              "change_percentage": {"type": "number"},
                              "trend": {
                                  "type": "string",
                                  "enum": ["up", "down", "stable"]
                              },
                              "unit": {"type": "string"}
                          }
                      }
                  },
                  "time_series": {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "properties": {
                              "timestamp": {"type": "string"},
                              "values": {
                                  "type": "object",
                                  "additionalProperties": {"type": "number"}
                              }
                          }
                      }
                  },
                  "segments": {
                      "type": "array",
                      "items": {
                          "type": "object",
                          "properties": {
                              "name": {"type": "string"},
                              "value": {"type": "number"},
                              "percentage": {"type": "number"},
                              "color": {"type": "string"}
                          }
                      }
                  }
              },
              "required": ["summary", "metrics"]
          }
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please format analytics data into standard format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Focus indicators:{', '.join(metrics)}
                      
                      Requirements:
                      - Calculate change percentage and trend
                      - Provide key insights
                      - Time series data sorted by time
                      - Segments data contain percentage
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Raw data: \n{raw_data}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Analytics data formatting failed: {e}")
              return None

  # Usage example
  formatter = APIResponseFormatter("your_api_key")

  # Format search results
  raw_search = """
  1. Python Programming Tutorial - https://example.com/python-tutorial
     Detailed introduction to Python basic syntax and programming concepts...

  2. Python Data Analysis Practice - https://example.com/python-data
     Using pandas and numpy for data processing...
  """

  formatted_results = formatter.format_search_results("Python Tutorial", raw_search)
  if formatted_results:
      print(f"Found {formatted_results['total_results']} results")
      for result in formatted_results['results']:
          print(f"Title: {result['title']}")
          print(f"Relevance: {result['relevance_score']}")
  ```
</Accordion>

<Accordion title="Configuration Management and Validation Complete Implementation">
  ```python  theme={null}
  class ConfigurationManager:
      def __init__(self, api_key):
          self.client = ZaiClient(api_key=api_key)

      def parse_config_file(self, config_text, config_type="general"):
          """Parse configuration file"""
          schemas = {
              "database": {
                  "type": "object",
                  "properties": {
                      "connections": {
                          "type": "array",
                          "items": {
                              "type": "object",
                              "properties": {
                                  "name": {"type": "string"},
                                  "host": {"type": "string"},
                                  "port": {"type": "integer"},
                                  "database": {"type": "string"},
                                  "username": {"type": "string"},
                                  "ssl": {"type": "boolean"},
                                  "pool_size": {"type": "integer"}
                              },
                              "required": ["name", "host", "database"]
                          }
                      },
                      "settings": {
                          "type": "object",
                          "properties": {
                              "timeout": {"type": "integer"},
                              "retry_attempts": {"type": "integer"},
                              "log_level": {
                                  "type": "string",
                                  "enum": ["DEBUG", "INFO", "WARNING", "ERROR"]
                              }
                          }
                      }
                  },
                  "required": ["connections"]
              },
              "api": {
                  "type": "object",
                  "properties": {
                      "endpoints": {
                          "type": "array",
                          "items": {
                              "type": "object",
                              "properties": {
                                  "name": {"type": "string"},
                                  "url": {"type": "string"},
                                  "method": {
                                      "type": "string",
                                      "enum": ["GET", "POST", "PUT", "DELETE"]
                                  },
                                  "headers": {"type": "object"},
                                  "timeout": {"type": "integer"},
                                  "rate_limit": {"type": "integer"}
                              },
                              "required": ["name", "url", "method"]
                          }
                      },
                      "authentication": {
                          "type": "object",
                          "properties": {
                              "type": {
                                  "type": "string",
                                  "enum": ["bearer", "basic", "api_key"]
                              },
                              "credentials": {"type": "object"}
                          }
                      }
                  },
                  "required": ["endpoints"]
              }
          }
          
          schema = schemas.get(config_type, {
              "type": "object",
              "additionalProperties": True
          })
          
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please parse the configuration file and convert to JSON format:
                      {json.dumps(schema, indent=2, ensure_ascii=False)}
                      
                      Configuration type: {config_type}
                      
                      Requirements:
                      - Identify configuration items and values
                      - Convert data types (string, number, boolean)
                      - Handle arrays and nested objects
                      - Validate required fields
                      - Provide default values (if applicable)
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Configuration file content:\n{config_text}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              validate(instance=result, schema=schema)
              return result
          except Exception as e:
              print(f"Configuration parsing failed: {e}")
              return None
      
      def validate_configuration(self, config_data, validation_rules):
          """Validate configuration"""
          response = self.client.chat.completions.create(
              model="glm-4.7",
              messages=[
                  {
                      "role": "system",
                      "content": f"""
                      Please validate configuration data and return validation results:
                      
                      Return format:
                      {{
                          "is_valid": true/false,
                          "errors": [
                              {{
                                  "field": "field_name",
                                  "error": "error_description",
                                  "severity": "error/warning/info"
                              }}
                          ],
                          "warnings": [
                              {{
                                  "field": "field_name",
                                  "message": "warning_message"
                              }}
                          ],
                          "suggestions": [
                              "improvement_suggestion_1",
                              "improvement_suggestion_2"
                          ]
                      }}
                      
                      Validation rules: {validation_rules}
                      """
                  },
                  {
                      "role": "user",
                      "content": f"Configuration data:\n{json.dumps(config_data, indent=2, ensure_ascii=False)}"
                  }
              ],
              response_format={"type": "json_object"}
          )
          
          try:
              result = json.loads(response.choices[0].message.content)
              return result
          except Exception as e:
              print(f"Configuration validation failed: {e}")
              return None

  # Usage example
  config_manager = ConfigurationManager("your_api_key")

  # Parse database configuration
  db_config_text = """
  [database]
  host = localhost
  port = 5432
  database = myapp
  username = admin
  ssl = true
  pool_size = 10

  [settings]
  timeout = 30
  retry_attempts = 3
  log_level = INFO
  """

  config = config_manager.parse_config_file(db_config_text, "database")
  if config:
      print("Parsed configuration:", json.dumps(config, indent=2, ensure_ascii=False))
      
      # Validate configuration
      validation_rules = [
          "Port number must be in range 1-65535",
          "Database name cannot be empty",
          "Connection pool size should be greater than 0",
          "Timeout should be reasonable (1-300 seconds)"
      ]
      
      validation_result = config_manager.validate_configuration(config, validation_rules)
      if validation_result:
          print(f"Configuration validity: {validation_result['is_valid']}")
          if validation_result['errors']:
              print("Errors:", validation_result['errors'])
          if validation_result['warnings']:
              print("Warnings:", validation_result['warnings'])
  ```
</Accordion>

## Best Practices

<CardGroup cols={2}>
  <Card title="Schema Design Principles" icon="code">
    * Clarity: Field names and types should be clear and explicit
    * Completeness: Include all necessary validation rules
    * Flexibility: Consider future expansion needs
  </Card>

  <Card title="Error Handling Strategy" icon="shield-check">
    * Multi-layer validation: Schema validation + business logic validation
    * Fallback plan: Prepare simplified backup Schema
    * Logging: Record detailed error information
  </Card>
</CardGroup>

<Warning>
  JSON mode requires AI to strictly output according to specified format, but in some complex scenarios it may affect the naturalness of responses. It's recommended to find a balance between functionality and user experience.
</Warning>

<Tip>
  When designing JSON Schema, it's recommended to start with simple structures and gradually increase complexity. Also, providing detailed descriptions and examples for key fields helps AI better understand and generate JSON data that meets requirements.
</Tip>


---
