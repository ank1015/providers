import type { Model } from "./types.js";

export const MODELS = {
	openai: {
		"gpt-5.2": {
			id: "gpt-5.2",
			name: "GPT-5.2",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 1.75,
				output: 14,
				cacheRead: 0.175,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
		"gpt-5.2-pro": {
			id: "gpt-5.2-pro",
			name: "GPT-5.2 Pro",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 21,
				output: 168,
				cacheRead: 21,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
		"gpt-5.1-codex-max": {
			id: "gpt-5.1-codex-max",
			name: "GPT-5.1 Codex Max",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 1.25,
				output: 10,
				cacheRead: 0.125,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
		"gpt-5-mini": {
			id: "gpt-5-mini",
			name: "GPT-5 Mini",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 0.25,
				output: 2,
				cacheRead: 0.025,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
		"gpt-5-nano": {
			id: "gpt-5-nano",
			name: "GPT-5 Nano",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 0.05,
				output: 0.4,
				cacheRead: 0.005,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
		"gpt-5": {
			id: "gpt-5",
			name: "GPT-5",
			api: "openai",
			baseUrl: "https://api.openai.com/v1",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 1.25,
				output: 10,
				cacheRead: 0.125,
				cacheWrite: 0,
			},
			contextWindow: 400000,
			maxTokens: 128000,
			tools: ['function_calling'],
		} satisfies Model<"openai">,
	},
	google: {
		"gemini-3-pro-preview": {
			id: "gemini-3-pro-preview",
			name: "Gemini 3 Pro Preview",
			api: "google",
			baseUrl: "https://generativelanguage.googleapis.com/v1beta",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 2,
				output: 12,
				cacheRead: 0.2,
				cacheWrite: 0,
			},
			contextWindow: 1048576,
			maxTokens: 65536,
			tools: ['function_calling'],
		} satisfies Model<"google">,
		"gemini-3-flash-preview": {
			id: "gemini-3-flash-preview",
			name: "Gemini 3 Flash Preview",
			api: "google",
			baseUrl: "https://generativelanguage.googleapis.com/v1beta",
			reasoning: true,
			input: ["text", "image", "file"],
			cost: {
				input: 0.50,
				output: 3,
				cacheRead: 0.05,
				cacheWrite: 0,
			},
			contextWindow: 1048576,
			maxTokens: 65536,
			tools: ['function_calling'],
		} satisfies Model<"google">,
		"gemini-3-pro-image-preview": {
			id: "gemini-3-pro-image-preview",
			name: "Gemini 3 Pro Image Preview",
			api: "google",
			baseUrl: "https://generativelanguage.googleapis.com/v1beta",
			reasoning: true,
			input: ["text", "image"],
			cost: {
				input: 2,
				output: 12,
				cacheRead: 2,
				cacheWrite: 0,
			},
			contextWindow: 65536,
			maxTokens: 32768,
			tools: [],
		} satisfies Model<"google">,
	},
	deepseek: {
		"deepseek": {
			id: "deepseek-reasoner",
			name: "Deepseek V3.2",
			api: "deepseek",
			baseUrl: "https://api.deepseek.com",
			reasoning: true,
			input: ["text"],
			cost: {
				input: 0.28,
				output: 0.42,
				cacheRead: 0.028,
				cacheWrite: 0,
			},
			contextWindow: 128000,
			maxTokens: 64000,
			tools: ['function_calling'],
		} satisfies Model<"deepseek">,
	}
}